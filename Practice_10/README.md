# Practice10_ZarsayevYevgeniy

## Отчет по Practice10


## Задание 1. Анализ производительности CPU-параллельной программы (OpenMP)

В этом задании требовалось реализовать параллельную обработку большого массива данных с использованием OpenMP, включая вычисление суммы, среднего значения и дисперсии. Программа была разработана с возможностью изменения числа потоков для выполнения вычислений. Для оценки производительности использовалась функция omp_get_wtime(), что позволило измерить время выполнения отдельных этапов (сумма и дисперсия) и общее время параллельного выполнения. Основная цель — изучить влияние числа потоков на ускорение и оценить долю последовательной и параллельной частей программы в контексте закона Амдала.

### Результаты

<img width="1185" height="139" alt="image" src="https://github.com/user-attachments/assets/ae511bec-788d-4f30-90cc-6cdbe2c611f2" />


Анализ результатов показывает, что при увеличении числа потоков ускорение незначительное и даже иногда наблюдается рост времени выполнения. Например, общее время на 2 потоках составляет 0.0867 с, на 4 потоках — 0.0902 с, а на 8 потоках — 0.0979 с. Это объясняется тем, что последовательная часть программы (инициализация, контроль и синхронизация потоков) ограничивает максимальное ускорение, как предсказывает закон Амдала. Для данной задачи вычислительная нагрузка на каждый элемент невелика, поэтому накладные расходы на управление потоками начинают доминировать, и увеличение числа потоков не приводит к значительному сокращению времени выполнения. Таким образом, для небольших задач оптимальное число потоков ограничено и требует балансировки между параллельной нагрузкой и накладными расходами.

### Блок-схема

<img width="345" height="764" alt="image" src="https://github.com/user-attachments/assets/bbc01598-baab-4abd-a60a-b16098501122" />

## Задание 2. Оптимизация доступа к памяти на GPU (CUDA)

В этом задании требовалось реализовать несколько версий CUDA-ядра для обработки массива данных, демонстрируя влияние различных паттернов доступа к памяти. Были реализованы ядра с коалесцированным и некоалесцированным доступом, а также с оптимизацией через shared memory и изменение организации потоков. Для каждой версии измерялось время выполнения с использованием cudaEvent, что позволило оценить эффективность разных подходов к доступу к глобальной и локальной памяти GPU. Основная цель - понять, как организация памяти и паттерны доступа влияют на производительность.

### Результаты


<img width="416" height="137" alt="image" src="https://github.com/user-attachments/assets/e2ad9151-f1ca-4697-b7dc-766d2bdeec29" />

Анализ результатов показывает сильную зависимость времени выполнения от способа доступа к памяти. Ядро с коалесцированным доступом к глобальной памяти оказалось медленнее (11.0046 мс) по сравнению с версией с некоалесцированным доступом (0.002688 мс) - здесь, вероятно, эффект связан с небольшим объёмом данных и оптимизацией компилятора. Использование shared memory и изменение организации потоков дополнительно сократило время до 0.002496 и 0.002432 мс соответственно, демонстрируя, что эффективное использование локальной памяти и правильная организация потоков могут значительно уменьшить накладные расходы доступа к глобальной памяти и повысить производительность GPU. Таким образом, для задач с интенсивной работой с памятью критически важно выбирать подходящий паттерн доступа и использовать локальные буферы.


### Блок-схема

<img width="395" height="800" alt="image" src="https://github.com/user-attachments/assets/981ee6ef-c7d1-46fb-bd72-80b4ac32ac2f" />


## Задание 3. Профилирование гибридного приложения CPU + GPU

В этом задании требовалось реализовать гибридную программу, в которой часть вычислений выполняется на CPU, а часть - на GPU, с использованием асинхронной передачи данных (cudaMemcpyAsync) и CUDA streams. Программа должна была измерять накладные расходы на передачу данных и выявлять узкие места при взаимодействии CPU и GPU. Основная цель - оценить, какие этапы ограничивают производительность, и предложить оптимизацию для снижения этих накладных расходов.

### Результаты


<img width="464" height="285" alt="image" src="https://github.com/user-attachments/assets/70b9e4ae-fc87-4e4d-b8e1-4f527bf19123" />


Анализ результатов показывает, что до оптимизации основным узким местом была последовательная передача данных между CPU и GPU: копирование на GPU и обратно занимало суммарно около 28,5 мс из общего времени 35,86 мс, тогда как вычисления на GPU - только 7,37 мс. После оптимизации с наложением вычислений CPU и GPU (overlap) время GPU-вычислений сократилось до 0,0147 мс, а общее время уменьшилось до 28,89 мс. Это демонстрирует, что оптимизация, позволяющая выполнять вычисления CPU параллельно с передачей данных на GPU, существенно снижает накладные расходы и повышает эффективность гибридной программы. Вывод: для гибридных приложений критически важно минимизировать простои GPU и CPU, используя асинхронные операции и наложение вычислений.


### Блок-схема

<img width="345" height="802" alt="image" src="https://github.com/user-attachments/assets/83c387b2-5e36-4b8b-81c7-ca2a91a6d593" />


## Задание 4. Анализ масштабируемости распределённой программы (MPI)


В этом задании требовалось реализовать распределённую программу на MPI для вычисления агрегатной функции над большим массивом и оценить её производительность при разном числе процессов. Основное внимание уделялось измерению времени выполнения операций MPI_Reduce и MPI_Allreduce, а также анализу масштабируемости алгоритма при увеличении числа процессов. Цель - понять, как коммуникационные операции влияют на эффективность и где возникают практические ограничения при распределённых вычислениях.


### Результаты


<img width="592" height="420" alt="image" src="https://github.com/user-attachments/assets/21993c57-8fe3-41c0-a6b5-0b7e4f658d2e" />


Анализ результатов показывает, что время выполнения операций сильно зависит от типа коммуникации. MPI_Reduce показал рост времени с увеличением числа процессов (от 3.37e-05 с на 2 процессах до 0.00526 с на 8 процессах), что связано с накоплением накладных расходов при последовательном объединении данных. MPI_Allreduce, использующий алгоритм с более эффективным обменом данными между процессами, показал значительно меньшее время (от 8.96e-06 с до 0.00017 с), демонстрируя преимущество коллективной синхронизации. Вывод: алгоритм хорошо масштабируется при небольшом числе процессов, однако при большом числе процессов коммуникационные операции могут стать узким местом. Для практических распределённых вычислений важно оптимизировать обмен данными и выбирать эффективные коллективные операции для поддержания производительности.


### Блок-схема

<img width="274" height="834" alt="image" src="https://github.com/user-attachments/assets/44d520a3-b176-4e2b-8ce9-aebab15d0e3b" />


## Выводы

В ходе выполнения практических заданий была исследована эффективность различных методов параллельных и распределённых вычислений на CPU, GPU и в гибридных системах. Задания показали, что производительность современных вычислительных систем сильно зависит от архитектуры, способа организации вычислений и доступа к памяти.

Анализ CPU-параллельных программ (OpenMP) показал, что увеличение числа потоков не всегда приводит к ускорению, особенно для задач с небольшой вычислительной нагрузкой на элемент, так как последовательная часть программы и накладные расходы на синхронизацию ограничивают эффект ускорения (закон Амдала). Для GPU критично эффективное использование памяти: паттерны доступа и применение shared memory позволяют существенно снизить время выполнения. Гибридные программы CPU+GPU демонстрируют, что оптимизация передачи данных и наложение вычислений CPU и GPU позволяют минимизировать узкие места и накладные расходы.

В распределённых вычислениях на MPI ключевым фактором является влияние коммуникационных операций: выбор эффективных коллективных функций (MPI_Allreduce вместо MPI_Reduce) позволяет снизить время синхронизации и улучшить масштабируемость. В целом, работа показала, что производительность параллельных и распределённых приложений зависит не только от вычислительной мощности, но и от правильной организации потоков, памяти и взаимодействия между узлами. Для оптимизации вычислений важно балансировать нагрузку, минимизировать накладные расходы и использовать возможности асинхронного и конкурентного выполнения.

# Контрольные вопросы


1. В чём отличие измерения времени выполнения от профилирования?


Ответ: Измерение времени выполнения показывает, сколько времени программа работает в целом или на отдельных участках. Профилирование идёт глубже и показывает, где именно тратится время: какие функции самые медленные и где возникают задержки.


2. Какие виды узких мест характерны для CPU, GPU и распределённых программ?


Ответ: Для CPU это может быть ограниченное количество ядер, частые ветвления и ожидание памяти. Для GPU - медленный доступ к памяти, плохая загрузка потоков и лишние копирования данных. В распределённых программах узким местом чаще всего становится обмен данными между узлами и задержки сети.


3. Почему увеличение числа потоков или процессов не всегда приводит к ускорению?
   

Ответ: Потому что растут накладные расходы на создание, синхронизацию и обмен данными. Кроме того, всегда есть часть программы, которая выполняется последовательно и не может быть распараллелена.


4. Как законы Амдала и Густафсона применяются при анализе масштабируемости?
   

Ответ: Закон Амдала показывает, что ускорение ограничено долей последовательного кода и бесконечного ускорения добиться нельзя. Закон Густафсона говорит, что при увеличении размера задачи параллельная часть растёт, и масштабируемость может быть лучше на больших данных.


5. Какие факторы наиболее критичны для производительности гибридных приложений?


Ответ: Критичны правильное распределение работы между CPU и GPU, минимизация передачи данных между ними и эффективное использование памяти. Также важно учитывать баланс нагрузки, чтобы ни CPU, ни GPU не простаивали.


### Сборка

!g++ -fopenmp task1_openmp.cpp -o task1_openmp

!nvcc task2_cuda.cu -o task2_cuda

!nvcc task3_hybrid.cu -o task3_hybrid

!mpicxx task4_mpi.cpp -o task4_mpi

### Запуск

!./task1_openmp

!./task2_cuda

!./task3_hybrid

!mpirun --allow-run-as-root --oversubscribe -np 2 ./task4_mpi
